<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Is the best good enough?</title>
    <meta charset="utf-8" />
    <meta name="author" content="Joshua Loftus (LSE Statistics)" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/ninjutsu.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: bottom, left, title-slide

.title[
# Is the best good enough?
]
.author[
### Joshua Loftus (LSE Statistics)
]

---











&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 1.2rem;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;

---

### High level intro

### Concluding thoughts

---


## Regression example: F-tests (of unselected variables)


- Regression models `\(E[Y] = X_A \beta_A\)` for some subset `\(A\)` of columns of a matrix `\(X\)`.
- With nested subsets `\(A \subsetneq A'\)`, we'll conduct an `\(F\)`-test and consider this as a goodness-of-fit test for the model with variables `\(A\)`.
- In `R` we just use the `anova` function with these two linear models.
- Econometrics: model specification test, omitted/included variable bias


The distribution of the `\(F\)`-statistic is derived, of course, under the assumption that `\(A\)` and `\(A'\)` have been chosen *a priori*

---

### High-dimensional setting
When many predictors are available, we often use (automated) model selection procedures like forward stepwise or the LASSO (Tibshirani, 1996) to choose a "good" subset


## Regression variable selection


- For concreteness, consider selecting variables using forward stepwise with BIC, i.e. in `R` with `step(..., k = log(n))`.
- Simulation with `\(n = 100\)` observations of `\(p = 10\)` variables, the first two coefficients are larger than the next 3, and the last 5 are all 0.
- In this (low-dimensional) example, we'll take `\(A' = \{ 1, \ldots, 10 \}\)` for simplicity.
- Consider the `\(F\)`-test as a goodness-of-fit test for the selected `\(A \subset A'\)`, and compute both unadjusted (classical) and adjusted (selective) p-values.

---

## Profile of model selection events
  

&lt;embed src="fstepbic2.pdf" type="application/pdf"&gt;

## Distributions of `\(p\)`-values for full-model `\(F\)`-tests
  \centering
  \begin{figure}
  \includegraphics[width=.99\textwidth]{fdists_both2}
  \end{figure}

---

## Probability of rejection

\begin{table}

\caption{\label{tab:1}Probability of rejection at level 0.1, conditional on size of overlap}

![](lowdimFtest.png)

\end{table}

---

## Conditional power

\begin{table}

\caption{\label{tab:}Probability of rejection at level 0.1, conditional on overlap less than 5}
\centering
\begin{tabular}[t]{l|r}
\hline
pvalue &amp; Pr(reject)\\
\hline
Naive &amp; 0.022\\
\hline
Adjusted &amp; 0.186\\
\hline
\end{tabular}
\end{table}

## Model selection bias

- Model selection bias invalidates many hypothesis tests
- Previously, **significance** of selected variables: **anti-conservative**
- This work on **goodness of fit** tests: **conservative**
- *Conditional on selecting wrong model, goodness-of-fit tests have low power*

Intuitively obvious: use the data to pick the model, and then use the same data to answer "does this model fit the data?"

\ 

Next: one solution approach (used to compute adjusted p-values in regression example)

---

# Post-selection inference

---

## Conditioning on selection

- `\(M = M(y)\)` model selection (e.g. marginal screening, forward stepwise)
- Observe `\(M = m\)`
- Test `\(H_0(m)\)` with statistic `\(T\)` conditioned on `\(\{ y : M(y) = m \}\)`

**Selective type 1 error** (Fithian, Sun, Taylor, 2014)

Reject `\(H_0(m)\)` if `\(T &gt; c_{\alpha}(m)\)`, where

`$$P_{H_{0}(m)}(T &gt; c_\alpha(m) \mid M = m) \leq \alpha$$`


In practice we may also condition on other things (sufficient statistics for nuisance parameters)

Growing literature (including e.g. details of truncation regions) for various selection methods in various settings

---

##  Related literature

- Early works by Olshen (1973), Hurvich &amp; Tsai (1990), Benjamini &amp; Yekutieli (2005) used basic idea of conditioning to adjust for selection

- Methods controlling FDR or similar: "Screen &amp; clean" Wasserman &amp; Roeder (2009), "Stability selection" Meinshausen &amp; Bühlmann (2010), an Empirical Bayes approach Efron (2011), "SLOPE" Bogdan et al. (2014), and "Knockoffs" Barber &amp; Candès (2015)

- Controlling type 1 error: Debiasing methods Bühlmann (2013); Javanmard &amp; Montanari (2014); Zhang &amp; Zhang (2014), Causal inference for univariate treatment Belloni et al. (2014), PoSI: simultaneous for all submodels Berk et al. (2013), Impossibility results: Leeb &amp; Pötscher (2005, 2006)

---

## More closely related literature

- Lasso, sequential Lockhart et al. (2014)
- General penalty, global null, geometry *Taylor et al. (2015)*; Azaïs et al. (2015)
- Forward stepwise, sequential *Loftus &amp; Taylor (2014)*
- Matrices: PCA/CCA Choi et al. (2014)
- **Fixed `\(\lambda\)` Lasso / conditional Lee et al. (2015)**
- Framework, optimality Fithian et al. (2014)
- Forward stepwise and LAR Tibshirani et al. (2014)
- Unknown `\(\sigma^2\)` *Tian et al. (2018)*; Gross et al. (2015)
- **Group selection / unknown `\(\sigma^2\)` *Loftus &amp; Taylor (2015)* **
- Cross-validation Tian &amp; Taylor (2015); *Loftus (2015)*; Markovic et al. (2017)

---

# Selective unbiasedness
 
---

## When the best isn't good enough

- We've seen a few examples but it should be clear there are far more we haven't mentioned
- Many combinations of model selection procedures and goodness of fit tests (or other diagnostics)
- In many such examples, such a test has power less than `\(\alpha\)` conditional on selecting the wrong model

Pretty troubling! Goodness-of-fit tests worse at detecting the wrong model has been selected than just tossing an `\(\alpha\)`-coin... What can we do?

---

## Selective unbiasedness

**Selective unbiasedness** (Fithian, Sun, Taylor, 2014)}
We say that a test is selectively unbiased if for any selected model `\(m\)` and alternative hypothesis `\(H_1(m)\)`,
`$$P_{H_1(m)}(\text{reject } H_0 | M = m) \geq \alpha$$`

We achieve this by using the conditional (truncated) distribution of the test statistic (if we can derive/compute it). *Same method used to control selective type 1 error will make (some) goodness-of-fit tests selectively unbiased*

---

## Marginal screening example

Consider a simpler problem of selecting marginal effects. From many independent effects, screen out those with small observed values, i.e. select those with large values (in this case bigger than 1)


```r
Z &lt;- rnorm(10000)
unselected_Z &lt;- data.frame(Z = Z[abs(Z) &lt; 1])
```

These are generated under the global null, and `\(Z : |Z| &gt; 1\)` are selected. How can we test the "goodness of fit" of selecting these effects? Test based on unselected effects.

---

#### Conditional null distribution is truncated: `\(Z | |Z| &lt; 1\)`


```r
truncated_Z_pdf &lt;- function(z) dnorm(z)/(pnorm(1)-pnorm(-1))
# plot code hidden
```

&lt;img src="IMS2022_files/figure-html/unnamed-chunk-3-1.png" width="504" /&gt;

---

### Interpretation/storytelling

- Keep model selection and inference compartmentalized

- The procedure/algorithm selects model `\(m\)`, this is evidence in favor of the model

- If model `\(m\)` contains variable `\(j\)`, this is evidence in favor of variable `\(j\)`

- We may still fail to reject the null hypothesis that `\(\beta_{m,j} = 0\)`, "the best model includes this variable but we cannot reject the null for it"

- Or we may reject the goodness of fit null: "this is the best model (in the class) but it still has specification problems"

"This is the best model we could find, and these inferences may be useful for understanding its limitations and directing future study"

---

## Follow-up study after Benjamini-Hochberg

**Idea**: after using the BH(*q*) procedure to select a subset of hypotheses while controlling the false discovery rate, decide whether to conduct a follow-up study of the hypotheses that were *not* selected

Use the conditional distribution of the non-rejected `\(p_{(j)}\)` to adjust e.g. Fisher's combination test

Interesting meta-analysis implications

---

## Power of Fisher's combination test

\begin{table}

\caption{\label{tab:1}Probability of rejection at level 0.05 with different Beta($1, \mu$)}

\centering

\begin{tabular}{cccc}

{$\mu$ } &amp; {$p_{1|0}$} &amp; unadjusted &amp; adjusted\\

 \hline

 10 &amp;  97 &amp;1.0000 &amp;1.0000\\

20  &amp; 87 &amp;0.9614 &amp;1.0000\\

 30  &amp; 60&amp; 0.5091 &amp;0.9838\\

 40   &amp;33 &amp;0.0900 &amp;0.8580\\

 50  &amp; 19 &amp;0.0040 &amp;0.6080\\

 60  &amp; 11 &amp;0.0000 &amp;0.3320\\

70   &amp; 6&amp; 0.0000 &amp;0.2004\\

  80   &amp; 4&amp; 0.0000&amp; 0.1245\\

90   &amp; 3&amp; 0.0000 &amp;0.0995\\

\end{tabular}

\end{table}

\small{$p_{1|0}$ denotes the average proportion of true nonnulls that are not rejected}

---

# Conclusion

---

## The "quiet scandal" in statistics (Leo Breiman)

People use the data to make modeling choices

This can bias (all sorts of) inferences

Conditioning on those choices and using truncated distributions is a simple idea with wide applicability

## Thank you!

Preprint should be posted soon, more procedures added to `selectiveInference` package on CRAN as we progress.


[joshualoftus.com](joshualoftus.com)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
(function(time) {
  var d2 = function(number) {
    return ('0' + number).slice(-2); // left-pad 0 to minutes/seconds
  },

  time_format = function(total) {
    var secs = Math.abs(total) / 1000;
    var h = Math.floor(secs / 3600);
    var m = Math.floor(secs % 3600 / 60);
    var s = Math.round(secs % 60);
    var res = d2(m) + ':' + d2(s);
    if (h > 0) res = h + ':' + res;
    return res;  // [hh:]mm:ss
  },

  slide_number_div = function(i) {
    return document.getElementsByClassName('remark-slide-number').item(i);
  },

  current_page_number = function(i) {
    return slide_number_div(i).firstChild.textContent;  // text "i / N"
  };

  var timer = document.createElement('span'); timer.id = 'slide-time-left';
  var time_left = time, k = slideshow.getCurrentSlideIndex(),
      last_page_number = current_page_number(k);

  setInterval(function() {
    time_left = time_left - 1000;
    timer.innerHTML = ' ' + time_format(time_left);
    if (time_left < 0) timer.style.color = 'red';
  }, 1000);

  slide_number_div(k).appendChild(timer);

  slideshow.on('showSlide', function(slide) {
    var i = slide.getSlideIndex(), n = current_page_number(i);
    // reset timer when a new slide is shown and the page number is changed
    if (last_page_number !== n) {
      time_left = time; last_page_number = n;
      timer.innerHTML = ' ' + time_format(time); timer.style.color = null;
    }
    slide_number_div(i).appendChild(timer);
  });
})(59000);
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
