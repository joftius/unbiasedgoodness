---
title: "Model selection bias invalidates goodness of fit tests"
author: "Joshua Loftus"
institute: "New York University"
output:
  beamer_presentation:
    theme: "Boadilla"
    colortheme: "seahorse"
    slide_level: 2
    toc: true
classoption: "handout"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})
library(tidyverse)
library(selectiveInference)
library(ggthemes)
library(stringr)
library(knitr)
set.seed(19)
```

# Motivating example


## Regression example: F-tests (of unselected variables)


- Regression models $E[Y] = X_A \beta_A$ for some subset $A$ of columns of a matrix $X$.
- With nested subsets $A \subsetneq A'$, we'll conduct an $F$-test and consider this as a goodness-of-fit test for the model with variables $A$.
- In \texttt{R} we just use the \texttt{anova} function with these two linear models.


The distribution of the $F$-statistic is derived, of course, under the assumption that $A$ and $A'$ have been chosen \textit{a priori}... 

\ 

(We briefly mentioned an idea like this in Tian et al. (2018))

## Regression variable selection


- For concreteness, consider selecting variables using forward stepwise with BIC, i.e. in \texttt{R} with \texttt{step(..., k = log(n))}.
- Simulation with $n = 100$ observations of $p = 10$ variables, the first two coefficients are larger than the next 3, and the last 5 are all 0.
- In this low-dimensional example, we'll take $A' = \{ 1, \ldots, 10 \}$ for simplicity.
- Consider the $F$-test as a goodness-of-fit test for the selected $A \subset A'$, and compute both unadjusted (classical) and adjusted (selective) p-values.


## Profile of model selection events
  \centering
  \includegraphics[width=.99\textwidth]{fstepbic2.pdf}


## Distributions of $p$-values for full-model $F$-tests
  \centering
  \begin{figure}
  \includegraphics[width=.99\textwidth]{fdists_both2}
  \caption{Top: unadjusted $p$-values. Bottom: adjusted for selection.}
  \end{figure}

## Probability of rejection

\begin{table}

\caption{\label{tab:1}Probability of rejection at level 0.1, conditional on size of overlap}
\centering
\begin{tabular}[t]{l|r|r}
\hline
pvalue & overlap & Pr(reject)\\
\hline
Naive & 2 & 0.056\\
\hline
Naive & 3 & 0.032\\
\hline
Naive & 4 & 0.017\\
\hline
Naive & 5 & 0.005\\
\hline
Adjusted & 2 & 0.328\\
\hline
Adjusted & 3 & 0.251\\
\hline
Adjusted & 4 & 0.156\\
\hline
Adjusted & 5 & 0.101\\
\hline
\end{tabular}
\end{table}

## Conditional power

\begin{table}

\caption{\label{tab:}Probability of rejection at level 0.1, conditional on overlap less than 5}
\centering
\begin{tabular}[t]{l|r}
\hline
pvalue & Pr(reject)\\
\hline
Naive & 0.022\\
\hline
Adjusted & 0.186\\
\hline
\end{tabular}
\end{table}



# General frameworks for post-selection inference



##  Related literature

- Early works by Olshen (1973), Hurvich & Tsai (1990), Benjamini & Yekutieli (2005) used basic idea of conditioning to adjust for selection

- Methods controlling FDR or similar: "Screen & clean" Wasserman & Roeder (2009), "Stability selection" Meinshausen & Bühlmann (2010), an Empirical Bayes approach Efron (2011), "SLOPE" Bogdan et al. (2014), and "Knockoffs" Barber & Candès (2015)

- Controlling type 1 error: Debiasing methods Bühlmann (2013); Javanmard & Montanari (2014); Zhang & Zhang (2014), Causal inference for univariate treatment Belloni et al. (2014), PoSI: simultaneous for all submodels Berk et al. (2013), Impossibility results: Leeb & Pötscher (2005, 2006)

## More closely related literature

- Lasso, sequential Lockhart et al. (2014)
- General penalty, global null, geometry *Taylor et al. (2015)*; Azaïs et al. (2015)
- Forward stepwise, sequential *Loftus & Taylor (2014)*
- Matrices: PCA/CCA Choi et al. (2014)
- **Fixed $\lambda$ Lasso / conditional Lee et al. (2015)**
- Framework, optimality Fithian et al. (2014)
- Forward stepwise and LAR Tibshirani et al. (2014)
- Unknown $\sigma^2$ *Tian et al. (2018)*; Gross et al. (2015)
- **Group selection / unknown $\sigma^2$ *Loftus & Taylor (2015)* **
- Cross-validation Tian & Taylor (2015); *Loftus (2015)*; Markovic et al. (2017)


# Selective unbiasedness and goodness-of-fit examples

## Direction of selection bias

Selection bias can result in tests that are anticonservative *or* conservative

\ 

Significance tests for selected predictors are generally biased toward rejection

\ 

What about goodness-of-fit tests?
 
## When the best isn't good enough

Use data to select the best model

\ 

Use same data to test whether the model fits... \pause

\ 

In many examples, such a test has power less than $\alpha$ conditional on selecting the wrong model
 
<!-- ## Simple time series example: the AR($p$) process -->

<!-- \begin{block}{Autoregressive model set up} -->
<!-- An AR($p$) process $\{Y_t\}$ with mean $\mu$ satisfies -->
<!-- \begin{equation*} -->
<!-- (Y_t-\mu) -\phi_1(Y_{t-1}-\mu)-...-\phi_p(Y_{t-p}-\mu)=W_t -->
<!-- \end{equation*} -->
<!-- where $W_t\sim\mathcal{N}(0,\sigma^2)$. For simplicity, we consider the zero-mean autoregressive process with $\mu = 0$ -->
<!-- \begin{equation*} -->
<!-- Y_t -\phi_1Y_{t-1}-...-\phi_pY_{t-p}=W_t, -->
<!-- \end{equation*} -->
<!-- where $W_t\sim\mathcal{N}(0,\sigma^2)$.  -->
<!-- \end{block} -->

<!-- ## Example AR(p) paths -->
<!--   \centering -->
<!--   \begin{figure} -->
<!--   \includegraphics[width=.9\textwidth]{ar_paths.pdf} -->
<!--   \caption{Top panel: AR(1), bottom panel: AR(2)} -->
<!--   \end{figure} -->


<!-- ## Goodness of fit test -->

<!-- Perhaps the most commonly used goodness of fit test in time series: -->
<!-- \begin{block}{Ljung-Box test (Ljung \& Box, 1978)} -->
<!-- The Ljung-Box statistic with $l$ lags is defined as -->
<!-- \begin{equation*} -->
<!-- Q^{(l)}(\widehat{r})=n(n+2)\sum_{k=1}^l(n-k)^{-1}\widehat{\mathrm{r}}(k)^2 -->
<!-- \end{equation*} -->
<!-- where -->
<!-- \begin{equation*} -->
<!-- \widehat{\mathrm{r}}(k)=\dfrac{\sum_{t=k+1}^n(Y_t-\widehat{Y}_t)(Y_{t-k}-\widehat{Y}_{t-k})}{\sum_{t=1}^n(Y_t-\widehat{Y}_t)^2},\;\;\;k=1,...,m -->
<!-- \end{equation*} -->
<!-- is the autocorrelation function. -->
<!-- \end{block} -->

<!-- If the data is truly AR($p$) but we fit AR($q$), with $q < p$, the LB test is powered to detect the residual$^2$ autocorrelation. -->

<!-- ## Observed LB vs null distribution, with selection -->
<!-- \begin{figure}[H] -->
<!-- \begin{center} -->
<!-- \includegraphics[width=.9\textwidth]{true3fit2_selected} -->
<!-- \caption{Truth is AR(3). Top row: when $\mathrm{AIC_C}$ selects $p = 3$. Bottom row: when $p = 2$ is selected. Distributions shown for LB with lags 5, 10, 15.} -->
<!-- \end{center} -->
<!-- \end{figure} -->

## Selective unbiasedness

Pretty troubling! Goodness-of-fit tests worse at detecting the wrong model has been selected than just tossing an $\alpha$-coin...

\begin{block}{Selective unbiasedness (Fithian, Sun, Taylor, 2014)}
We say that a test is selectively unbiased if for any selected model $m$ and alternative hypothesis $H_1(m)$,
$$
P_{H_1(m)}(\text{reject } H_0 | M = m) \geq \alpha
$$
\end{block}

We achieve this by using the conditional (truncated) distribution of the test statistic (if we can derive/compute it...)

## Marginal screening example

Consider a simpler problem of selecting marginal effects. From many independent effects, screen out those with small observed values, i.e. select those with large values (in this case bigger than 1)

```{r cache=TRUE}
Z <- rnorm(10000)
unselected_Z <- data.frame(Z = Z[abs(Z) < 1])
```

These are generated under the global null, and $Z : |Z| > 1$ are selected. How can we test the "goodness of fit" of selecting these effects? Test based on unselected effects.

## Conditional null distribution is truncated: $Z | |Z| < 1$

```{r mysize=TRUE, size='\\footnotesize',}
truncated_Z_pdf <- function(z) dnorm(z)/(pnorm(1)-pnorm(-1))
# plot code hidden
```

```{r echo=F, cache=TRUE}
ggplot(unselected_Z) +
  geom_histogram(bins = 70, aes(x = Z, y = ..density..)) + xlim(-2, 2) +
  stat_function(fun = truncated_Z_pdf, xlim = c(-1, 1), linetype  = 2) +
  stat_function(fun = dnorm, linetype  = 1) +
  theme_tufte()
```

## Selective (conditional) inference 

- $M = M(y)$ model selection (e.g. marginal screening, forward stepwise)
- Observe $M = m$
- Test $H_0(m)$ with statistic $T$ conditioned on $\{ y : M(y) = m \}$

\begin{block}{Selective type 1 error (Fithian, Sun, Taylor, 2014)}
Reject $H_0(m)$ if $T > c_{\alpha}(m)$, where 
$$
P_{H_{0}(m)}(T > c_\alpha(m) \mid M = m) \leq \alpha
$$
\end{block}
\pause
In practice we may also condition on other things (sufficient statistics for nuisance parameters)

Growing literature (including e.g. details of truncation regions) for various selection methods in various settings

For various goodness of fit tests, adjusting them to control selective type 1 error will result in tests that are selectively unbiased (for some class of alternatives)

## Interpretation

- Keep model selection and inference compartmentalized

- The procedure/algorithm selects model $m$, this is a sort of evidence in favor of the model

- If model $m$ contains variable $j$, this is evidence in favor of variable $j$

- We may still fail to reject the null hypothesis that $\beta_{m,j} = 0$

- This does not mean we should now go back and change the model! (See first point above)


## Follow-up study after Benjamini-Hochberg

**Idea**: after using the BH($q$) procedure to select a subset of hypotheses while controlling the false discovery rate, decide whether to conduct a follow-up study of the hypotheses that were *not* selected

\ 

Use the conditional distribution of the non-rejected $p_{(j)}$ to adjust e.g. Fisher's combination test

\ 

Interesting meta-analysis implications

## Power of Fisher's combination test

\begin{table}

\caption{\label{tab:1}Probability of rejection at level 0.05 with different Beta($1, \mu$)}

\centering

\begin{tabular}{cccc}

{$\mu$ } & {$p_{1|0}$} & unadjusted & adjusted\\

 \hline

 10 &  97 &1.0000 &1.0000\\

20  & 87 &0.9614 &1.0000\\

 30  & 60& 0.5091 &0.9838\\

 40   &33 &0.0900 &0.8580\\

 50  & 19 &0.0040 &0.6080\\

 60  & 11 &0.0000 &0.3320\\

70   & 6& 0.0000 &0.2004\\

  80   & 4& 0.0000& 0.1245\\

90   & 3& 0.0000 &0.0995\\

\end{tabular}

\end{table}

\small{$p_{1|0}$ denotes the average proportion of true nonnulls that are not rejected}


# Conclusion

## The "quiet scandal" in statistics (Leo Breiman)

People use the data to make modeling choices

\ 

This can bias (all sorts of) inferences

\ 

Conditioning on those choices and using truncated distributions is a simple idea with wide applicability

## Thank you!

Preprint should be posted soon, more procedures added to `selectiveInference` package on CRAN as we progress.


\ 

\centering

[joshualoftus.com](joshualoftus.com)
