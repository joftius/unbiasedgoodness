@Manual{rSI,
  title = {selectiveInference: Tools for Post-Selection Inference},
  author = {Ryan Tibshirani and Rob Tibshirani and Jonathan Taylor and Joshua Loftus and Stephen Reid and Jelena Markovic},
  year = {2019},
  note = {R package version 1.2.5},
  url = {https://CRAN.R-project.org/package=selectiveInference},
}

@Manual{rRPtests,
  title = {RPtests: Goodness of Fit Tests for High-Dimensional Linear Regression
    Models},
  author = {Rajen Shah and Peter Buhlmann},
  year = {2021},
  note = {R package version 0.1.5},
  url = {https://CRAN.R-project.org/package=RPtests},
}

@article{tukey1960,
  title = {Conclusions vs {Decisions}},
  volume = {2},
  issn = {0040-1706},
  url = {http://www.jstor.org/stable/1266451},
  doi = {10.2307/1266451},
  abstract = {With the exception of appendices 2 and 3, the following is based on the after dinner talk given by Professor John W. Tukey at the first meeting of the Section of the Physical and Engineering Sciences of the American Statistical Association held in New York City on May 26, 1955. This talk was repeated at a later date before a dinner meeting of the Metropolitan Section of the American Society for Quality Control. On both occasions considerable discussion ensued. The talk is published here both for the record, and in the hope that some readers may be stimulated to prepare written rejoinders.},
  number = {4},
  urldate = {2022-08-22},
  journal = {Technometrics},
  author = {Tukey, John W.},
  year = {1960},
  note = {Publisher: [Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
  pages = {423--433},
  file = {Tukey_1960_Conclusions vs Decisions.pdf:/Users/loftus/Dropbox/reading/zotero/Tukey_1960_Conclusions vs Decisions.pdf:application/pdf},
}

@article{lehmann2012,
  title = {The {Fisher}, {Neyman}-{Pearson} {Theories} of {Testing} {Hypotheses}: {One} {Theory} or {Two}?},
  copyright = {Copyright Taylor and Francis Group, LLC},
  shorttitle = {The {Fisher}, {Neyman}-{Pearson} {Theories} of {Testing} {Hypotheses}},
  url = {https://link.springer.com/content/pdf/10.1007/978-1-4614-1412-4_19.pdf},
  abstract = {The Fisher and Neyman-Pearson approaches to testing statistical hypotheses are compared with respect to their attitudes to the interpretation of the outcome, to power, to conditioning, and to the u...},
  language = {en},
  urldate = {2022-08-22},
  journal = {Journal of the American Statistical Association},
  author = {Lehmann, E. L.},
  month = feb,
  year = {2012},
  note = {Publisher: Taylor \& Francis Group},
  file = {Lehmann_2012_The Fisher, Neyman-Pearson Theories of Testing Hypotheses.pdf:/Users/loftus/Dropbox/reading/zotero/Lehmann_2012_The Fisher, Neyman-Pearson Theories of Testing Hypotheses.pdf:application/pdf;Snapshot:/Users/loftus/Zotero/storage/25K5ISSE/01621459.1993.html:text/html},
}

@article{shah2018,
  title = {Goodness-of-fit tests for high dimensional linear models},
  volume = {80},
  issn = {1467-9868},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12234},
  doi = {10.1111/rssb.12234},
  abstract = {We propose a framework for constructing goodness-of-fit tests in both low and high dimensional linear models. We advocate applying regression methods to the scaled residuals following either an ordinary least squares or lasso fit to the data, and using some proxy for prediction error as the final test statistic. We call this family residual prediction tests. We show that simulation can be used to obtain the critical values for such tests in the low dimensional setting and demonstrate using both theoretical results and extensive numerical studies that some form of the parametric bootstrap can do the same when the high dimensional linear model is under consideration. We show that residual prediction tests can be used to test for significance of groups or individual variables as special cases, and here they compare favourably with state of the art methods, but we also argue that they can be designed to test for as diverse model misspecifications as heteroscedasticity and non-linearity.},
  language = {en},
  number = {1},
  urldate = {2022-08-24},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  author = {Shah, Rajen D. and Bühlmann, Peter},
  year = {2018},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12234},
  pages = {113--135},
  file = {rssb12234-sup-0001-supinfo.pdf:/Users/loftus/Zotero/storage/NXMJKKBX/rssb12234-sup-0001-supinfo.pdf:application/pdf;Shah_Bühlmann_2018_Goodness-of-fit tests for high dimensional linear models.pdf:/Users/loftus/Dropbox/reading/zotero/inference/Shah_Bühlmann_2018_Goodness-of-fit tests for high dimensional linear models.pdf:application/pdf;Snapshot:/Users/loftus/Zotero/storage/FGZTI48F/rssb.html:text/html},
}

@article{jankova2020,
  title = {Goodness-of-fit testing in high dimensional generalized linear models},
  volume = {82},
  issn = {1467-9868},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12371},
  doi = {10.1111/rssb.12371},
  abstract = {We propose a family of tests to assess the goodness of fit of a high dimensional generalized linear model. Our framework is flexible and may be used to construct an omnibus test or directed against testing specific non-linearities and interaction effects, or for testing the significance of groups of variables. The methodology is based on extracting left-over signal in the residuals from an initial fit of a generalized linear model. This can be achieved by predicting this signal from the residuals by using modern powerful regression or machine learning methods such as random forests or boosted trees. Under the null hypothesis that the generalized linear model is correct, no signal is left in the residuals and our test statistic has a Gaussian limiting distribution, translating to asymptotic control of type I error. Under a local alternative, we establish a guarantee on the power of the test. We illustrate the effectiveness of the methodology on simulated and real data examples by testing goodness of fit in logistic regression models. Software implementing the methodology is available in the R package GRPtests.},
  language = {en},
  number = {3},
  urldate = {2022-08-24},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  author = {Janková, Jana and Shah, Rajen D. and Bühlmann, Peter and Samworth, Richard J.},
  year = {2020},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12371},
  pages = {773--795},
  file = {Janková et al_2020_Goodness-of-fit testing in high dimensional generalized linear models.pdf:/Users/loftus/Dropbox/reading/zotero/inference/Janková et al_2020_Goodness-of-fit testing in high dimensional generalized linear models.pdf:application/pdf;Snapshot:/Users/loftus/Zotero/storage/UMHWQU5C/rssb.html:text/html},
}

@article{sun2012,
  title = {Scaled sparse linear regression},
  volume = {99},
  issn = {0006-3444, 1464-3510},
  url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/ass043},
  doi = {10.1093/biomet/ass043},
  abstract = {Scaled sparse linear regression jointly estimates the regression coefficients and noise level in a linear model. It chooses an equilibrium with a sparse regression method by iteratively estimating the noise level via the mean residual square and scaling the penalty in proportion to the estimated noise level. The iterative algorithm costs little beyond the computation of a path or grid of the sparse regression estimator for penalty levels above a proper threshold. For the scaled lasso, the algorithm is a gradient descent in a convex minimization of a penalized joint loss function for the regression coefficients and noise level. Under mild regularity conditions, we prove that the scaled lasso simultaneously yields an estimator for the noise level and an estimated coefficient vector satisfying certain oracle inequalities for prediction, the estimation of the noise level and the regression coefficients. These inequalities provide sufficient conditions for the consistency and asymptotic normality of the noise-level estimator, including certain cases where the number of variables is of greater order than the sample size. Parallel results are provided for least-squares estimation after model selection by the scaled lasso. Numerical results demonstrate the superior performance of the proposed methods over an earlier proposal of joint convex minimization.},
  language = {en},
  number = {4},
  urldate = {2022-08-24},
  journal = {Biometrika},
  author = {Sun, T. and Zhang, C.-H.},
  month = dec,
  year = {2012},
  pages = {879--898},
  file = {Sun and Zhang - 2012 - Scaled sparse linear regression.pdf:/Users/loftus/Zotero/storage/KZNXFG2J/Sun and Zhang - 2012 - Scaled sparse linear regression.pdf:application/pdf},
}

@article{zhang2014,
  title = {Confidence intervals for low dimensional parameters in high dimensional linear models},
  volume = {76},
  issn = {1467-9868},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12026},
  doi = {10.1111/rssb.12026},
  abstract = {The purpose of this paper is to propose methodologies for statistical inference of low dimensional parameters with high dimensional data. We focus on constructing confidence intervals for individual coefficients and linear combinations of several of them in a linear regression model, although our ideas are applicable in a much broader context. The theoretical results that are presented provide sufficient conditions for the asymptotic normality of the proposed estimators along with a consistent estimator for their finite dimensional covariance matrices. These sufficient conditions allow the number of variables to exceed the sample size and the presence of many small non-zero coefficients. Our methods and theory apply to interval estimation of a preconceived regression coefficient or contrast as well as simultaneous interval estimation of many regression coefficients. Moreover, the method proposed turns the regression data into an approximate Gaussian sequence of point estimators of individual regression coefficients, which can be used to select variables after proper thresholding. The simulation results that are presented demonstrate the accuracy of the coverage probability of the confidence intervals proposed as well as other desirable properties, strongly supporting the theoretical results.},
  language = {en},
  number = {1},
  urldate = {2022-08-24},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  author = {Zhang, Cun-Hui and Zhang, Stephanie S.},
  year = {2014},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12026},
  pages = {217--242},
  file = {Full Text PDF:/Users/loftus/Zotero/storage/XI827TMX/Zhang and Zhang - 2014 - Confidence intervals for low dimensional parameter.pdf:application/pdf;Snapshot:/Users/loftus/Zotero/storage/K8PGU2CJ/rssb.html:text/html},
}

@article{goeman2006,
  title = {Testing against a high dimensional alternative},
  volume = {68},
  issn = {1467-9868},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2006.00551.x},
  doi = {10.1111/j.1467-9868.2006.00551.x},
  abstract = {Summary. As the dimensionality of the alternative hypothesis increases, the power of classical tests tends to diminish quite rapidly. This is especially true for high dimensional data in which there are more parameters than observations. We discuss a score test on a hyperparameter in an empirical Bayesian model as an alternative to classical tests. It gives a general test statistic which can be used to test a point null hypothesis against a high dimensional alternative, even when the number of parameters exceeds the number of samples. This test will be shown to have optimal power on average in a neighbourhood of the null hypothesis, which makes it a proper generalization of the locally most powerful test to multiple dimensions. To illustrate this new locally most powerful test we investigate the case of testing the global null hypothesis in a linear regression model in more detail. The score test is shown to have significantly more power than the F-test whenever under the alternative the large variance principal components of the design matrix explain substantially more of the variance of the outcome than do the small variance principal components. The score test is also useful for detecting sparse alternatives in truly high dimensional data, where its power is comparable with the test based on the maximum absolute t-statistic.},
  language = {en},
  number = {3},
  urldate = {2022-08-24},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  author = {Goeman, Jelle J. and Van De Geer, Sara A. and Van Houwelingen, Hans C.},
  year = {2006},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2006.00551.x},
  pages = {477--493},
  file = {Full Text PDF:/Users/loftus/Zotero/storage/UPT783TJ/Goeman et al. - 2006 - Testing against a high dimensional alternative.pdf:application/pdf;Snapshot:/Users/loftus/Zotero/storage/JDVAECH3/j.1467-9868.2006.00551.html:text/html},
}

@article{dezeure2015,
  title = {High-{Dimensional} {Inference}: {Confidence} {Intervals}, p-{Values} and {R}-{Software} hdi},
  volume = {30},
  issn = {0883-4237},
  shorttitle = {High-{Dimensional} {Inference}},
  url = {https://www.jstor.org/stable/24780819},
  abstract = {We present a (selective) review of recent frequentist high-dimensional inference methods for constructing p-values and confidence intervals in linear and generalized linear models. We include a broad, comparative empirical study which complements the viewpoint from statistical methodology and theory. Furthermore, we introduce and illustrate the R-package hdi which easily allows the use of different methods and supports reproducibility.},
  number = {4},
  urldate = {2022-08-24},
  journal = {Statistical Science},
  author = {Dezeure, Ruben and Bühlmann, Peter and Meier, Lukas and Meinshausen, Nicolai},
  year = {2015},
  note = {Publisher: Institute of Mathematical Statistics},
  pages = {533--558},
  file = {JSTOR Full Text PDF:/Users/loftus/Zotero/storage/LAE76XCX/Dezeure et al. - 2015 - High-Dimensional Inference Confidence Intervals, .pdf:application/pdf},
}

@article{buhlmann2014,
  title = {High-{Dimensional} {Statistics} with a {View} {Toward} {Applications} in {Biology}},
  volume = {1},
  issn = {2326-8298, 2326-831X},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-022513-115545},
  doi = {10.1146/annurev-statistics-022513-115545},
  abstract = {We review statistical methods for high-dimensional data analysis and pay particular attention to recent developments for assessing uncertainties in terms of controlling false positive statements (type I error) and p-values. The main focus is on regression models, but we also discuss graphical modeling and causal inference based on observational data. We illustrate the concepts and methods with various packages from the statistical software R using a high-throughput genomic data set about riboﬂavin production with Bacillus subtilis, which we make publicly available for the ﬁrst time.},
  language = {en},
  number = {1},
  urldate = {2022-08-24},
  journal = {Annual Review of Statistics and Its Application},
  author = {Bühlmann, Peter and Kalisch, Markus and Meier, Lukas},
  month = jan,
  year = {2014},
  pages = {255--278},
  file = {Bühlmann et al. - 2014 - High-Dimensional Statistics with a View Toward App.pdf:/Users/loftus/Zotero/storage/4YS8D4NF/Bühlmann et al. - 2014 - High-Dimensional Statistics with a View Toward App.pdf:application/pdf},
}

@misc{shah2022,
  title = {Double-estimation-friendly inference for high-dimensional misspecified models},
  url = {http://arxiv.org/abs/1909.10828},
  doi = {10.48550/arXiv.1909.10828},
  abstract = {All models may be wrong -- but that is not necessarily a problem for inference. Consider the standard \$t\$-test for the significance of a variable \$X\$ for predicting response \$Y\$ whilst controlling for \$p\$ other covariates \$Z\$ in a random design linear model. This yields correct asymptotic type{\textasciitilde}I error control for the null hypothesis that \$X\$ is conditionally independent of \$Y\$ given \$Z\$ under an {\textbackslash}emph\{arbitrary\} regression model of \$Y\$ on \$(X, Z)\$, provided that a linear regression model for \$X\$ on \$Z\$ holds. An analogous robustness to misspecification, which we term the "double-estimation-friendly" (DEF) property, also holds for Wald tests in generalised linear models, with some small modifications. In this expository paper we explore this phenomenon, and propose methodology for high-dimensional regression settings that respects the DEF property. We advocate specifying (sparse) generalised linear regression models for both \$Y\$ and the covariate of interest \$X\$; our framework gives valid inference for the conditional independence null if either of these hold. In the special case where both specifications are linear, our proposal amounts to a small modification of the popular debiased Lasso test. We also investigate constructing confidence intervals for the regression coefficient of \$X\$ via inverting our tests; these have coverage guarantees even in partially linear models where the contribution of \$Z\$ to \$Y\$ can be arbitrary. Numerical experiments demonstrate the effectiveness of the methodology.},
  urldate = {2022-08-24},
  publisher = {arXiv},
  author = {Shah, Rajen D. and Bühlmann, Peter},
  month = may,
  year = {2022},
  note = {arXiv:1909.10828 [math, stat]},
  file = {arXiv Fulltext PDF:/Users/loftus/Zotero/storage/PDUPE73U/Shah and Bühlmann - 2022 - Double-estimation-friendly inference for high-dime.pdf:application/pdf;arXiv.org Snapshot:/Users/loftus/Zotero/storage/8W2Q4VEG/1909.html:text/html},
}



@Article{rhdi,
  title = {High-Dimensional Inference: Confidence Intervals, p-values and {R}-Software {hdi}},
  author = {Ruben Dezeure and Peter B\"uhlmann and Lukas Meier and Nicolai Meinshausen},
  journal = {Statistical Science},
  year = {2015},
  volume = {30},
  number = {4},
  pages = {533--558},
}

@Manual{rPoSI,
  title = {PoSI: Valid Post-Selection Inference for Linear LS Regression},
  author = {Andreas Buja and Kai Zhang},
  year = {2020},
  note = {R package version 1.1},
  url = {https://CRAN.R-project.org/package=PoSI},
}
