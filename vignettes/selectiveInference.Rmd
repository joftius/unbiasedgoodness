---
title: "selectiveInference"
output: rmarkdown::html_vignette
bibliography: bibliography.bib
link-citations: TRUE
vignette: >
  %\VignetteIndexEntry{selectiveInference}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{glmnet}
  %\VignetteDepends{tidyverse}
  %\VignetteDepends{purrr}
  %\VignetteDepends{hdi}
  %\VignetteDepends{unbiasedgoodness}
  %\VignetteDepends{selectiveInference}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Philosophy based on conditional inference


[**Does this specific selected model have a good fit, or does it fail to capture signal that an alternate model could detect?**]{.smallcaps}


## Low vs high-dimensions

The standard $F$-test for goodness-of-fit for an *a priori* fixed model compares its prediction error to an intercept only model. This is what is reported at the end of a `summary(lm(y ~ x))`:

```{r }
set.seed(1)
x <- runif(50)
y <- x + rnorm(50)
summary(lm(y ~ x))
```


### Lasso details


### `groupfs` code explanation


#### Summary


## Strategy for goodness-of-fit test

The original model selection algorithm chooses some number of variables, let's call this `s0`. To test for the presence of signal variables that were not included in this original fit, we choose a larger sparsity level `m * s0 + b` with `m >= 1` and `b > 0`. We use the model selection algorithm to include more variables up to this level of sparsity, and we test the significance of these new additional variables as a way of checking the fit of the original, smaller model.

## Simulations and examples

```{r setup, message=FALSE}
library(selectiveInference)
library(hdi) # riboflavin dataset
library(purrr)
library(tidyverse)
devtools::load_all() # this shouldn't be necessary?
library(unbiasedgoodness)
```

We continue focusing on the high-dimensional linear model and as an alternative to a selected model we consider whether more variables should be included. 

### One data generation example

```{r}
set.seed(1)
n <- 100
p <- 200
s0 <- 5
sim_data <- rXb(n, p, s0, xtype = "equi.corr")
x <- sim_data$x
beta <- sim_data$beta
y <- x %*% beta + rnorm(n)
# True support
which(beta != 0)
```


```{r}
ybar <- mean(y)
y <- y - ybar
data <- data.frame(y = y, x = x)
fit <- groupfs(x, y, index = 1:ncol(x), intercept = FALSE, center = FALSE, normalize = FALSE, aicstop = 1, k = log(ncol(x)))
against <- setdiff(fit$index, fit$action)
adjpv <- gof_groupfs(fit, against)
```


```{r}
mselected <- lm(as.formula(paste("y~-1+", paste(paste0("x.", fit$action), collapse="+"))), data = data)
mfull <- lm(y ~ .-1, data = data)
unadjpv <- anova(mselected, mfull)[2,c(3,5,6)]
vars <- sort(fit$action)
```

### Iterate many times

```{r}
#devtools::load_all() 
#library(unbiasedgoodness)
set.seed(1)
n <- 100
p <- 200
s0 <- 5
m <- 1.2
b <- 10

simulate_gof_instance(m, b, n, p, s0)
```

```{r glmnet-RPtest-simulation}
#devtools::load_all() 
#library(unbiasedgoodness)
set.seed(1)
niters <- 1000
sim_results_file <- "./data/sim_groupfs.csv"

if (file.exists(sim_results_file)) {
  output <- read.csv(sim_results_file)
  print(paste0(
    "Loading saved simulation results with ",
    nrow(output), " iterates"))
} else {
  # run simulation
  output <- simulate_gof(niters, m, b, n, p, s0)
  write.csv(output, sim_results_file)
}
```

### Distributions of $p$-values

```{r groupfs-pval-plots, echo=FALSE, message=FALSE, results='hide'}
output |>
  pivot_longer(
    starts_with("pval"),
    names_to = "type") |>
  group_by(type) |>
  group_map(~ ggplot(.) + aes(value) + 
    xlab("p-value") + 
    ylab("ecdf") +
    ggtitle(.y[[1]]) +
    stat_ecdf() + xlim(0, 1) + ylim(0,1) +
    geom_abline(slope = 1, linetype = "dashed") +
    facet_grid(~ null)
  ) |>
  print()
```

### Selective type 1 error rate

```{r}
output |>
  pivot_longer(
    starts_with("pval"),
    names_to = "type") |>
  group_by(type, null) |>
  summarize(error_rate = mean(value < 0.05), .groups = "keep")
```

```{r}
output |>
  dplyr::filter(null == TRUE) |>
  pivot_longer(
    starts_with("pval"),
    names_to = "type") |>
  group_by(type) |>
  summarize(error_rate = mean(value < 0.05), .groups = "keep")
```


### Selective power

```{r}
output |>
  dplyr::filter(null == FALSE) |>
  pivot_longer(
    starts_with("pval"),
    names_to = "type") |>
  group_by(type) |>
  summarize(power = mean(value < 0.05), .groups = "keep")
```

## Model selection success rates

These diagnostics are computed to give some indication of the difficulty level of lasso model selection for the chosen data generation process.

```{r}
output |> 
  summarize(null = mean(null),
            alt = mean(alt),
            s_null = mean(s_null),
            s_alt = mean(s_alt))
```

```{r}
output %>% pull(null_intersection) %>% qplot()
output %>% pull(alt_intersection) %>% qplot()
```

```{r}
output |> pull(s_alt) |> qplot()
```

```{r}
output |> ggplot(aes(max_corr, null_intersection)) + 
  geom_point() + geom_smooth(method = "lm")
```

```{r}
output |> ggplot(aes(min_beta, null_intersection)) + 
  geom_point() + geom_smooth(method = "lm")
```

```{r}
output %>% pull(null_intersection) |> qplot()
```



## References
